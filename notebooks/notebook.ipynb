{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import mapie\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import xgboost\n",
    "from IPython.display import clear_output, display\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pymongo import MongoClient\n",
    "from pymongoarrow.api import find_pandas_all\n",
    "from sklearn import (compose, dummy, ensemble, impute, linear_model, metrics,\n",
    "                     model_selection, neighbors, pipeline, preprocessing, svm,\n",
    "                     tree)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MongoClient(creds.Creds.URI)\n",
    "\n",
    "query = {\"building_condition\": \"Good\"}\n",
    "df = find_pandas_all(cluster.dev.BE_houses, None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show choropleth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BE_provinces = requests.get(\n",
    "    \"https://raw.githubusercontent.com/mathiasleroy/Belgium-Geographic-Data/master/dist/polygons/be-provinces-unk-WGS84.geo.json\"\n",
    ").json()\n",
    "\n",
    "aggregate = (\n",
    "    df.assign(list_price=lambda df: pd.to_numeric(df.price))\n",
    "    .groupby(\"province\")\n",
    "    .agg(\n",
    "        list_price_count=(\"price\", \"count\"),\n",
    "        list_price_mean=(\"price\", \"median\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.choropleth(\n",
    "    aggregate,\n",
    "    geojson=BE_provinces,\n",
    "    locations=\"province\",\n",
    "    color=\"list_price_mean\",\n",
    "    featureidkey=\"properties.name\",\n",
    "    projection=\"mercator\",\n",
    "    color_continuous_scale=\"Magenta\",\n",
    "    labels={\n",
    "        \"list_price_mean\": \"Median Price\",\n",
    "        \"list_price_count\": \"Number of Observations\",\n",
    "    },\n",
    "    hover_data={\"list_price_mean\": \":.3s\", \"province\": True, \"list_price_count\": True},\n",
    ")\n",
    "\n",
    "fig.update_geos(\n",
    "    showcountries=True, showcoastlines=True, showland=True, fitbounds=\"locations\"\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "fig.update_layout(\n",
    "    title_text=\"Median House Prices by Province\",\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    geo=dict(showframe=False, showcoastlines=False, projection_type=\"mercator\"),\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_map(data, feature):\n",
    "    BE_provinces = requests.get(\n",
    "        \"https://raw.githubusercontent.com/mathiasleroy/Belgium-Geographic-Data/master/dist/polygons/be-provinces-unk-WGS84.geo.json\"\n",
    "    ).json()\n",
    "    if is_numeric_dtype(data[feature]):\n",
    "        aggregate = data.groupby(\"province\")[feature].median().reset_index()\n",
    "        fig = px.choropleth(\n",
    "            aggregate,\n",
    "            geojson=BE_provinces,\n",
    "            locations=\"province\",\n",
    "            color=feature,\n",
    "            featureidkey=\"properties.name\",\n",
    "            projection=\"mercator\",\n",
    "            color_continuous_scale=\"Magenta\",\n",
    "        )\n",
    "\n",
    "        fig.update_geos(\n",
    "            showcountries=True,\n",
    "            showcoastlines=True,\n",
    "            showland=True,\n",
    "            fitbounds=\"locations\",\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Median {feature.replace('_', ' ').title()} by Province\",\n",
    "            coloraxis_colorbar_title_text=f\"{feature.replace('_', ' ').title()}\",\n",
    "            autosize=False,\n",
    "            width=800,\n",
    "            height=600,\n",
    "            geo=dict(showframe=False, showcoastlines=False, projection_type=\"mercator\"),\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "\n",
    "show_map(df, \"construction_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_index(data):\n",
    "    BE_provinces = requests.get(\n",
    "        \"https://raw.githubusercontent.com/mathiasleroy/Belgium-Geographic-Data/master/dist/polygons/be-provinces-unk-WGS84.geo.json\"\n",
    "    ).json()\n",
    "    number_of_ads = (\n",
    "        data.groupby(\"province\")\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"number_of_ads\"})\n",
    "    )\n",
    "    fig = px.choropleth(\n",
    "        number_of_ads,\n",
    "        geojson=BE_provinces,\n",
    "        locations=\"province\",\n",
    "        color=\"number_of_ads\",\n",
    "        featureidkey=\"properties.name\",\n",
    "        projection=\"mercator\",\n",
    "        color_continuous_scale=\"RdBu_r\",\n",
    "    )\n",
    "\n",
    "    fig.update_geos(\n",
    "        showcountries=True, showcoastlines=True, showland=True, fitbounds=\"locations\"\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Number of Ads by Province\",\n",
    "        coloraxis_colorbar_title_text=\"Number of Ads\",\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        geo=dict(showframe=False, showcoastlines=False, projection_type=\"mercator\"),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "hot_index(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "* Remove properties below 100k\n",
    "* Split X and y\n",
    "* Convert y to log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_null = (\n",
    "    df.drop(columns=[\"_id\", \"ad_url\", \"day_of_retrieval\"])\n",
    "    .dropna(subset=\"price\")\n",
    "    .query(\"price >= 100000\")\n",
    ")\n",
    "\n",
    "y = np.log10(df_no_null[\"price\"])\n",
    "X = df_no_null.drop(columns=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=45\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=45\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Identify outliers in a DataFrame.\n",
    "\n",
    "    This function uses a Local Outlier Factor (LOF) algorithm to identify outliers in a given\n",
    "    DataFrame. It operates on both numerical and categorical features, and it returns a binary\n",
    "    Series where `True` represents an outlier and `False` represents a non-outlier.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing features for outlier identification.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: A Boolean Series indicating outliers (True) and non-outliers (False).\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    # Load your DataFrame with features (df)\n",
    "    df = load_data()\n",
    "\n",
    "    # Identify outliers using the function\n",
    "    outlier_mask = identify_outliers(df)\n",
    "\n",
    "    # Use the outlier mask to filter your DataFrame\n",
    "    filtered_df = df[~outlier_mask]  # Keep non-outliers\n",
    "    ```\n",
    "\n",
    "    Notes:\n",
    "    - The function uses Local Outlier Factor (LOF) with default parameters for identifying outliers.\n",
    "    - Numerical features are imputed using median values, and categorical features are one-hot encoded\n",
    "    and imputed with median values.\n",
    "    - The resulting Boolean Series is `True` for inliers and `False` for outliers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract numerical and categorical feature names\n",
    "    NUMERICAL_FEATURES = df.select_dtypes(\"number\").columns.tolist()\n",
    "    CATEGORICAL_FEATURES = df.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "    # Define transformers for preprocessing\n",
    "    numeric_transformer = pipeline.Pipeline(\n",
    "        steps=[(\"imputer\", impute.SimpleImputer(strategy=\"median\"))]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = pipeline.Pipeline(\n",
    "        steps=[\n",
    "            (\"encoder\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "    preprocessor = compose.ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUMERICAL_FEATURES),\n",
    "            (\"cat\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Initialize the LOF model\n",
    "    # clf = neighbors.LocalOutlierFactor()\n",
    "    clf = ensemble.IsolationForest()\n",
    "\n",
    "    # Fit LOF to preprocessed data and make predictions\n",
    "    y_pred = clf.fit_predict(preprocessor.fit_transform(df))\n",
    "\n",
    "    # Adjust LOF predictions to create a binary outlier mask\n",
    "    outlier_mask = pd.Series(y_pred) == 1\n",
    "\n",
    "    return outlier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_mask = identify_outliers(X_train)\n",
    "X_train.index = outlier_mask.index\n",
    "X_train_wo_outliers = X_train[outlier_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical and categorical feature names\n",
    "NUMERICAL_FEATURES = X_train_wo_outliers.select_dtypes(\"number\").columns.tolist()\n",
    "CATEGORICAL_FEATURES = X_train_wo_outliers.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "# Define transformers for preprocessing\n",
    "numeric_transformer = pipeline.Pipeline(\n",
    "    steps=[(\"imputer\", impute.SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "categorical_transformer = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a ColumnTransformer to handle both numerical and categorical features\n",
    "preprocessor = compose.ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, NUMERICAL_FEATURES),\n",
    "        (\"cat\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X_train_wo_outliers)\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train_transformed = preprocessor.transform(X_train_wo_outliers)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "model = catboost.CatBoostRegressor()\n",
    "\n",
    "# Fit the model on the transformed training data\n",
    "model.fit(\n",
    "    X_train_transformed,\n",
    "    y_train,\n",
    "    eval_set=[(X_val_transformed, y_val)],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=False,\n",
    "    use_best_model=True,\n",
    ")\n",
    "\n",
    "mapie_model = mapie.regression.MapieRegressor(model)\n",
    "# Fit the MAPIE model\n",
    "mapie_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions with prediction intervals on the transformed validation data\n",
    "y_pred, y_pis = mapie_model.predict(X_test_transformed, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with y_valid and prediction intervals\n",
    "conformal_df = 10 ** pd.DataFrame(\n",
    "    {\n",
    "        \"y_test\": y_test,\n",
    "        \"lower\": y_pis[:, 0].flatten(),\n",
    "        \"upper\": y_pis[:, 1].flatten(),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by y_valid\n",
    "df_sorted = conformal_df.sort_values(by=\"y_test\")\n",
    "\n",
    "# Plot data\n",
    "\n",
    "plt.scatter(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"y_pred\"],\n",
    "    color=\"red\",\n",
    "    label=\"predicted\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.scatter(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"y_test\"],\n",
    "    color=\"green\",\n",
    "    label=\"ground truth\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.fill_between(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"lower\"],\n",
    "    df_sorted[\"upper\"],\n",
    "    alpha=0.2,\n",
    "    color=\"gray\",\n",
    "    label=\"Prediction Intervals\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal_df.round(0).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost_train = catboost.Pool(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     cat_features=X.select_dtypes(include='object').columns.tolist(),\n",
    "# )\n",
    "\n",
    "# catboost_valid = catboost.Pool(\n",
    "#     X_valid,\n",
    "#     y_valid,\n",
    "#     cat_features=X.select_dtypes(include='object').columns.tolist(),\n",
    "# )\n",
    "\n",
    "# model = catboost.CatBoostRegressor(\n",
    "#     loss_function=\"RMSE\",\n",
    "# )\n",
    "model.fit(\n",
    "    catboost_train,\n",
    "    eval_set=[catboost_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=False,\n",
    "    use_best_model=True,\n",
    ")\n",
    "\n",
    "# mapie_model = mapie.regression.MapieRegressor(model, method=\"plus\")\n",
    "\n",
    "# # fit MAPIE model\n",
    "# mapie_model.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions with prediction intervals\n",
    "# y_pred, y_pis = mapie_model.predict(X_valid, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer for selecting specific columns from a DataFrame.\n",
    "\n",
    "    This class inherits from the BaseEstimator and TransformerMixin classes from sklearn.base.\n",
    "    It overrides the fit and transform methods from the parent classes.\n",
    "\n",
    "    Attributes:\n",
    "        feature_names_in_ (list): The names of the features to select.\n",
    "        n_features_in_ (int): The number of features to select.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y=None): Fit the transformer. Returns self.\n",
    "        transform(X, y=None): Apply the transformation. Returns a DataFrame with selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_names_in_):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the FeatureSelector object.\n",
    "\n",
    "        Args:\n",
    "            feature_names_in_ (list): The names of the features to select.\n",
    "        \"\"\"\n",
    "        self.feature_names_in_ = feature_names_in_\n",
    "        self.n_features_in_ = len(feature_names_in_)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer. This method doesn't do anything as no fitting is necessary.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Apply the transformation. Selects the features from the input data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with only the selected features.\n",
    "        \"\"\"\n",
    "        return X.loc[:, self.feature_names_in_].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log10(df_no_null[\"price\"])\n",
    "X = df_no_null.drop(columns=[\"price\", \"_id\", \"ad_url\", \"day_of_retrieval\"])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns by dtypes\n",
    "\n",
    "numerical_columns = X_train.head().select_dtypes(\"number\").columns.to_list()\n",
    "categorical_columns = X_train.head().select_dtypes(\"object\").columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pipelines for corresponding columns:\n",
    "numerical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"num_selector\", FeatureSelector(numerical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n",
    "        (\"std_scaler\", preprocessing.MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"cat_selector\", FeatureSelector(categorical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Put all the pipelines inside a FeatureUnion:\n",
    "data_preprocessing_pipeline = pipeline.FeatureUnion(\n",
    "    n_jobs=-1,\n",
    "    transformer_list=[\n",
    "        (\"numerical_pipeline\", numerical_pipeline),\n",
    "        (\"categorical_pipeline\", categorical_pipeline),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "    MLA = [\n",
    "        linear_model.LinearRegression(),\n",
    "        linear_model.SGDRegressor(),\n",
    "        linear_model.PassiveAggressiveRegressor(),\n",
    "        linear_model.RANSACRegressor(),\n",
    "        linear_model.Lasso(),\n",
    "        svm.SVR(),\n",
    "        ensemble.GradientBoostingRegressor(),\n",
    "        tree.DecisionTreeRegressor(),\n",
    "        ensemble.RandomForestRegressor(),\n",
    "        ensemble.ExtraTreesRegressor(),\n",
    "        ensemble.AdaBoostRegressor(),\n",
    "        catboost.CatBoostRegressor(silent=True),\n",
    "        lgb.LGBMRegressor(verbose=-1),\n",
    "        xgboost.XGBRegressor(verbosity=0),\n",
    "        dummy.DummyClassifier(),\n",
    "    ]\n",
    "\n",
    "    # note: this is an alternative to train_test_split\n",
    "    cv_split = model_selection.ShuffleSplit(\n",
    "        n_splits=5, test_size=0.3, train_size=0.6, random_state=0\n",
    "    )  # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "    # create table to compare MLA metrics\n",
    "    MLA_columns = [\n",
    "        \"MLA Name\",\n",
    "        \"MLA Parameters\",\n",
    "        \"MLA Train RMSE Mean\",\n",
    "        \"MLA Test RMSE Mean\",\n",
    "        \"MLA Train R2 Mean\",\n",
    "        \"MLA Test R2 Mean\",\n",
    "        \"MLA Time\",\n",
    "    ]\n",
    "    MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
    "\n",
    "    # index through MLA and save performance to table\n",
    "    row_index = 0\n",
    "    for alg in tqdm(MLA):\n",
    "        # set name and parameters\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, \"MLA Name\"] = MLA_name\n",
    "        MLA_compare.loc[row_index, \"MLA Parameters\"] = str(alg.get_params())\n",
    "\n",
    "        model_pipeline = pipeline.Pipeline(\n",
    "            steps=[\n",
    "                (\"data_preprocessing_pipeline\", data_preprocessing_pipeline),\n",
    "                (\"model\", alg),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cv_results = model_selection.cross_validate(\n",
    "            model_pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv_split,\n",
    "            scoring={\n",
    "                \"r2\": \"r2\",\n",
    "                \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "            },\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        MLA_compare.loc[row_index, \"MLA Time\"] = cv_results[\"fit_time\"].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Train RMSE Mean\"] = cv_results[\n",
    "            \"train_neg_root_mean_squared_error\"\n",
    "        ].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test RMSE Mean\"] = cv_results[\n",
    "            \"test_neg_root_mean_squared_error\"\n",
    "        ].mean()\n",
    "\n",
    "        MLA_compare.loc[row_index, \"MLA Train R2 Mean\"] = cv_results[\"train_r2\"].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test R2 Mean\"] = cv_results[\"test_r2\"].mean()\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(MLA_compare.sort_values(by=[\"MLA Test RMSE Mean\"], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
