{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import catboost\n",
    "import lightgbm as lgb\n",
    "import mapie\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import xgboost\n",
    "from IPython.display import clear_output, display\n",
    "from pymongo import MongoClient\n",
    "from pymongoarrow.api import find_pandas_all\n",
    "from sklearn import (compose, dummy, ensemble, impute, linear_model, metrics,\n",
    "                     model_selection, pipeline, preprocessing, svm, tree)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import creds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MongoClient(creds.Creds.URI)\n",
    "\n",
    "query = {\"building_condition\": \"Good\"}\n",
    "df = find_pandas_all(cluster.development.BE_houses, None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show choropleth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BE_provinces = requests.get(\n",
    "    \"https://raw.githubusercontent.com/mathiasleroy/Belgium-Geographic-Data/master/dist/polygons/be-provinces-unk-WGS84.geo.json\"\n",
    ").json()\n",
    "\n",
    "aggregate = (\n",
    "    df.assign(list_price=lambda df: pd.to_numeric(df.list_price))\n",
    "    .groupby(\"province\")\n",
    "    .agg(\n",
    "        list_price_count=(\"list_price\", \"count\"),\n",
    "        list_price_mean=(\"list_price\", \"median\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.choropleth(\n",
    "    aggregate,\n",
    "    geojson=BE_provinces,\n",
    "    locations=\"province\",\n",
    "    color=\"list_price_mean\",\n",
    "    featureidkey=\"properties.name\",\n",
    "    projection=\"mercator\",\n",
    "    color_continuous_scale=\"Magenta\",\n",
    "    labels={\n",
    "        \"list_price_mean\": \"Median Price\",\n",
    "        \"list_price_count\": \"Number of Observations\",\n",
    "    },\n",
    "    hover_data={\"list_price_mean\": \":.3s\", \"province\": True, \"list_price_count\": True},\n",
    ")\n",
    "\n",
    "fig.update_geos(\n",
    "    showcountries=True, showcoastlines=True, showland=True, fitbounds=\"locations\"\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "fig.update_layout(\n",
    "    title_text=\"Median House Prices by Province\",\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    "    geo=dict(showframe=False, showcoastlines=False, projection_type=\"mercator\"),\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for preparing data after loading from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df):\n",
    "    columns_to_keep = [\n",
    "        \"construction_year\",\n",
    "        \"building_condition\",\n",
    "        \"number_of_frontages\",\n",
    "        \"living_area\",\n",
    "        \"bedrooms\",\n",
    "        \"bathrooms\",\n",
    "        \"toilets\",\n",
    "        \"surface_of_the_plot\",\n",
    "        \"primary_energy_consumption\",\n",
    "        \"energy_class\",\n",
    "        \"heating_type\",\n",
    "        \"tenement_building\",\n",
    "        \"list_price\",\n",
    "        \"double_glazing\",\n",
    "        \"province\",\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        df.filter(columns_to_keep)\n",
    "        .assign(\n",
    "            construction_year=lambda df: pd.to_numeric(\n",
    "                df.construction_year, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            number_of_frontages=lambda df: pd.to_numeric(\n",
    "                df.number_of_frontages, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            living_area=lambda df: pd.to_numeric(\n",
    "                df.living_area, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            bedrooms=lambda df: pd.to_numeric(\n",
    "                df.bedrooms, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            bathrooms=lambda df: pd.to_numeric(\n",
    "                df.bathrooms, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            toilets=lambda df: pd.to_numeric(\n",
    "                df.toilets, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            surface_of_the_plot=lambda df: pd.to_numeric(\n",
    "                df.surface_of_the_plot, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            primary_energy_consumption=lambda df: pd.to_numeric(\n",
    "                df.primary_energy_consumption, downcast=\"unsigned\", errors=\"coerce\"\n",
    "            ),\n",
    "            list_price=lambda df: np.log10(\n",
    "                pd.to_numeric(df.list_price, downcast=\"unsigned\", errors=\"coerce\")\n",
    "            ),\n",
    "            building_condition=lambda df: df.building_condition.fillna(\n",
    "                \"Unknown\"\n",
    "            ).astype(\"category\"),\n",
    "            energy_class=lambda df: df.energy_class.fillna(\"Unknown\").astype(\n",
    "                \"category\"\n",
    "            ),\n",
    "            heating_type=lambda df: df.heating_type.fillna(\"Unknown\").astype(\n",
    "                \"category\"\n",
    "            ),\n",
    "            tenement_building=lambda df: df.tenement_building.fillna(\"Unknown\").astype(\n",
    "                \"category\"\n",
    "            ),\n",
    "            double_glazing=lambda df: df.double_glazing.fillna(\"Unknown\").astype(\n",
    "                \"category\"\n",
    "            ),\n",
    "            province=lambda df: df.province.fillna(\"Unknown\").astype(\"category\"),\n",
    "        )\n",
    "        .dropna(subset=[\"list_price\"])\n",
    "    )\n",
    "\n",
    "\n",
    "processed_df = prepare_data_for_modeling(df)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining conformity intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = processed_df[\"list_price\"]\n",
    "X = processed_df.drop(\n",
    "    columns=[\n",
    "        \"list_price\",\n",
    "        \"building_condition\",\n",
    "        \"energy_class\",\n",
    "        \"heating_type\",\n",
    "        \"tenement_building\",\n",
    "        \"double_glazing\",\n",
    "        \"province\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_train = catboost.Pool(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    # cat_features=X.select_dtypes(include='category').columns.tolist(),\n",
    ")\n",
    "\n",
    "catboost_valid = catboost.Pool(\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    # cat_features=X.select_dtypes(include='category').columns.tolist(),\n",
    ")\n",
    "\n",
    "model = catboost.CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "model.fit(\n",
    "    catboost_train,\n",
    "    eval_set=[catboost_valid],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=False,\n",
    "    use_best_model=True,\n",
    ")\n",
    "\n",
    "mapie_model = mapie.regression.MapieRegressor(model, method=\"plus\")\n",
    "\n",
    "# fit MAPIE model\n",
    "mapie_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions with prediction intervals\n",
    "y_pred, y_pis = mapie_model.predict(X_valid, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with y_valid and prediction intervals\n",
    "conformal_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_valid\": y_valid,\n",
    "        \"lower\": y_pis[:, 0].flatten(),\n",
    "        \"upper\": y_pis[:, 1].flatten(),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by y_valid\n",
    "df_sorted = conformal_df.sort_values(by=\"y_valid\")\n",
    "\n",
    "# Plot prediction intervals\n",
    "plt.fill_between(\n",
    "    df_sorted[\"y_valid\"],\n",
    "    df_sorted[\"lower\"],\n",
    "    df_sorted[\"upper\"],\n",
    "    alpha=0.5,\n",
    "    color=\"gray\",\n",
    "    label=\"Prediction Intervals\",\n",
    ")\n",
    "plt.scatter(\n",
    "    df_sorted[\"y_valid\"], df_sorted[\"y_pred\"], color=\"blue\", label=\"Predictions\"\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer for selecting specific columns from a DataFrame.\n",
    "\n",
    "    This class inherits from the BaseEstimator and TransformerMixin classes from sklearn.base.\n",
    "    It overrides the fit and transform methods from the parent classes.\n",
    "\n",
    "    Attributes:\n",
    "        feature_names_in_ (list): The names of the features to select.\n",
    "        n_features_in_ (int): The number of features to select.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y=None): Fit the transformer. Returns self.\n",
    "        transform(X, y=None): Apply the transformation. Returns a DataFrame with selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_names_in_):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the FeatureSelector object.\n",
    "\n",
    "        Args:\n",
    "            feature_names_in_ (list): The names of the features to select.\n",
    "        \"\"\"\n",
    "        self.feature_names_in_ = feature_names_in_\n",
    "        self.n_features_in_ = len(feature_names_in_)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer. This method doesn't do anything as no fitting is necessary.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            self: The instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Apply the transformation. Selects the features from the input data.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input data.\n",
    "            y (array-like, optional): The target variable. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame with only the selected features.\n",
    "        \"\"\"\n",
    "        return X.loc[:, self.feature_names_in_].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = prepare_data_for_modeling(df)\n",
    "y = processed_df[\"list_price\"]\n",
    "X = processed_df.drop(columns=[\"list_price\"])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns by dtypes\n",
    "\n",
    "numerical_columns = X_train.head().select_dtypes(\"number\").columns.to_list()\n",
    "categorical_columns = X_train.head().select_dtypes(\"category\").columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pipelines for corresponding columns:\n",
    "numerical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"num_selector\", FeatureSelector(numerical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"median\")),\n",
    "        (\"std_scaler\", preprocessing.MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = pipeline.Pipeline(\n",
    "    steps=[\n",
    "        (\"cat_selector\", FeatureSelector(categorical_columns)),\n",
    "        (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Put all the pipelines inside a FeatureUnion:\n",
    "data_preprocessing_pipeline = pipeline.FeatureUnion(\n",
    "    n_jobs=-1,\n",
    "    transformer_list=[\n",
    "        (\"numerical_pipeline\", numerical_pipeline),\n",
    "        (\"categorical_pipeline\", categorical_pipeline),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.RANSACRegressor(),\n",
    "    linear_model.Lasso(),\n",
    "    svm.SVR(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    tree.DecisionTreeRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    catboost.CatBoostRegressor(silent=True),\n",
    "    lgb.LGBMRegressor(verbose=-1),\n",
    "    xgboost.XGBRegressor(verbosity=0),\n",
    "    dummy.DummyClassifier(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "    MLA = [\n",
    "        linear_model.LinearRegression(),\n",
    "        linear_model.SGDRegressor(),\n",
    "        linear_model.PassiveAggressiveRegressor(),\n",
    "        linear_model.RANSACRegressor(),\n",
    "        linear_model.Lasso(),\n",
    "        svm.SVR(),\n",
    "        ensemble.GradientBoostingRegressor(),\n",
    "        tree.DecisionTreeRegressor(),\n",
    "        ensemble.RandomForestRegressor(),\n",
    "        ensemble.ExtraTreesRegressor(),\n",
    "        ensemble.AdaBoostRegressor(),\n",
    "        catboost.CatBoostRegressor(silent=True),\n",
    "        lgb.LGBMRegressor(verbose=-1),\n",
    "        xgboost.XGBRegressor(verbosity=0),\n",
    "        dummy.DummyClassifier(),\n",
    "    ]\n",
    "\n",
    "    # note: this is an alternative to train_test_split\n",
    "    cv_split = model_selection.ShuffleSplit(\n",
    "        n_splits=10, test_size=0.3, train_size=0.6, random_state=0\n",
    "    )  # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "    # create table to compare MLA metrics\n",
    "    MLA_columns = [\n",
    "        \"MLA Name\",\n",
    "        \"MLA Parameters\",\n",
    "        \"MLA Train RMSE Mean\",\n",
    "        \"MLA Test RMSE Mean\",\n",
    "        \"MLA Train R2 Mean\",\n",
    "        \"MLA Test R2 Mean\",\n",
    "        \"MLA Time\",\n",
    "    ]\n",
    "    MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
    "\n",
    "    # index through MLA and save performance to table\n",
    "    row_index = 0\n",
    "    for alg in tqdm(MLA):\n",
    "        # set name and parameters\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, \"MLA Name\"] = MLA_name\n",
    "        MLA_compare.loc[row_index, \"MLA Parameters\"] = str(alg.get_params())\n",
    "\n",
    "        model_pipeline = pipeline.Pipeline(\n",
    "            steps=[\n",
    "                (\"data_preprocessing_pipeline\", data_preprocessing_pipeline),\n",
    "                (\"model\", alg),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cv_results = model_selection.cross_validate(\n",
    "            model_pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv_split,\n",
    "            scoring={\n",
    "                \"r2\": \"r2\",\n",
    "                \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "            },\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        MLA_compare.loc[row_index, \"MLA Time\"] = cv_results[\"fit_time\"].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Train RMSE Mean\"] = cv_results[\n",
    "            \"train_neg_root_mean_squared_error\"\n",
    "        ].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test RMSE Mean\"] = cv_results[\n",
    "            \"test_neg_root_mean_squared_error\"\n",
    "        ].mean()\n",
    "\n",
    "        MLA_compare.loc[row_index, \"MLA Train R2 Mean\"] = cv_results[\"train_r2\"].mean()\n",
    "        MLA_compare.loc[row_index, \"MLA Test R2 Mean\"] = cv_results[\"test_r2\"].mean()\n",
    "\n",
    "        row_index += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(MLA_compare.sort_values(by=[\"MLA Test RMSE Mean\"], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scraped_data.pickle\", \"rb\") as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_zip_to_province(value):\n",
    "    # data from https://www.spotzi.com/en/data-catalog/categories/postal-codes/belgium/\n",
    "    if value is None:\n",
    "        return None\n",
    "\n",
    "    first_two_digits = int(value[:2])\n",
    "\n",
    "    province_dict = {\n",
    "        range(10, 13): \"Brussels\",\n",
    "        range(13, 15): \"Walloon Brabant\",\n",
    "        range(15, 20): \"Flemish Brabant\",\n",
    "        range(30, 35): \"Flemish Brabant\",\n",
    "        range(20, 30): \"Antwerp\",\n",
    "        range(35, 40): \"Limburg\",\n",
    "        range(40, 50): \"Liege\",\n",
    "        range(50, 60): \"Namur\",\n",
    "        range(60, 66): \"Hainaut\",\n",
    "        range(70, 80): \"Hainaut\",\n",
    "        range(66, 70): \"Luxembourg\",\n",
    "        range(80, 90): \"West Flanders\",\n",
    "        range(90, 100): \"East Flanders\",\n",
    "    }\n",
    "\n",
    "    for key in province_dict:\n",
    "        if first_two_digits in key:\n",
    "            return province_dict[key]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def reformat_entries(data):\n",
    "    return (\n",
    "        data.apply(lambda x: x.astype(str))\n",
    "        .assign(\n",
    "            price=lambda x: pd.to_numeric(\n",
    "                x[\"price\"].str.extract(r\"(\\d+,\\d+)\", expand=False).str.replace(\",\", \"\"),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            province=lambda x: x[\"zip_code\"].apply(convert_zip_to_province),\n",
    "            energy_class=lambda x: x[\"energy_class\"].str.strip(),\n",
    "            primary_energy_consumption=lambda x: pd.to_numeric(\n",
    "                x[\"primary_energy_consumption\"]\n",
    "                .str.extract(r\"(\\d+)\", expand=False)\n",
    "                .str.replace(\",\", \"\"),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            bedrooms=lambda x: pd.to_numeric(\n",
    "                x[\"bedrooms\"].str.extract(r\"(\\d+)\", expand=False), errors=\"coerce\"\n",
    "            ),\n",
    "            tenement_building=lambda x: x[\"tenement_building\"].str.strip(),\n",
    "            living_area=lambda x: pd.to_numeric(\n",
    "                x[\"living_area\"]\n",
    "                .str.extract(r\"(\\d+)\", expand=False)\n",
    "                .str.replace(\",\", \"\"),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            surface_of_the_plot=lambda x: pd.to_numeric(\n",
    "                x[\"surface_of_the_plot\"]\n",
    "                .str.extract(r\"(\\d+)\", expand=False)\n",
    "                .str.replace(\",\", \"\"),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            bathrooms=lambda x: pd.to_numeric(\n",
    "                x[\"bathrooms\"].str.extract(r\"(\\d+)\", expand=False), errors=\"coerce\"\n",
    "            ),\n",
    "            double_glazing=lambda x: x[\"double_glazing\"].str.strip(),\n",
    "            number_of_frontages=lambda x: pd.to_numeric(\n",
    "                x[\"number_of_frontages\"].str.extract(r\"(\\d+)\", expand=False),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "            building_condition=lambda x: x[\"building_condition\"].str.strip(),\n",
    "            toilets=lambda x: pd.to_numeric(\n",
    "                x[\"toilets\"].str.extract(r\"(\\d+)\", expand=False), errors=\"coerce\"\n",
    "            ),\n",
    "            heating_type=lambda x: x[\"heating_type\"].str.strip(),\n",
    "            construction_year=lambda x: pd.to_numeric(\n",
    "                x[\"construction_year\"].str.extract(r\"(\\d+)\", expand=False),\n",
    "                errors=\"coerce\",\n",
    "            ),\n",
    "        )\n",
    "        .transpose()\n",
    "        .squeeze()\n",
    "        .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = data[1].apply(lambda x: x.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_entries(data[0].apply(lambda x: x.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(data[i].construction_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapy_house",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
