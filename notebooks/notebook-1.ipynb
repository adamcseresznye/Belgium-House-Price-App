{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69412634-b259-4563-93c6-cf8064969b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import catboost\n",
    "import joblib\n",
    "import mapie\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from cleanlab import regression\n",
    "from scipy import stats\n",
    "from sklearn import (compose, feature_selection, impute, metrics,\n",
    "                     model_selection, pipeline, preprocessing)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import creds\n",
    "import data_processing\n",
    "import model\n",
    "import preprocessing_transformers\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c9950-3e8c-4b25-9774-558a068ab8e0",
   "metadata": {},
   "source": [
    "# Retrieve and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01bbdc-61df-4656-a86b-5aabfe45bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_processing.retrieve_data_from_MongoDB(\n",
    "    \"development\", \"BE_houses\", {\"day_of_retrieval\": \"2024-02-09\"}, \"_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e1b2-3e78-4612-a3fe-a29a543093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_processing.preprocess_and_split_data(df)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab66bf9-4c93-4909-a88e-0977824a3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Shape of X_train : {X_train.shape}, X_test : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4faed5b-139a-47d4-8f64-2999912c5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURES = X_train.select_dtypes(\"number\").columns.tolist()\n",
    "CATEGORICAL_FEATURES = X_train.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "print(NUMERICAL_FEATURES)\n",
    "print(CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db2b09-8efa-4118-aa12-228722c9bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in categorical columns:\")\n",
    "for column in X_train[CATEGORICAL_FEATURES]:\n",
    "    print(f\"{column} : {X_train[column].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d189685-e9ec-4de5-b55d-038cf64eb27c",
   "metadata": {},
   "source": [
    "# Define a basic pipeline to use for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03948e-2467-4e07-a2a6-03b89b5b2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(\n",
    "    numerical_features, categorical_features, additional_transformers=None\n",
    "):\n",
    "    numeric_transformer = pipeline.make_pipeline(\n",
    "        impute.SimpleImputer(strategy=\"median\")\n",
    "    )\n",
    "\n",
    "    categorical_transformer = pipeline.make_pipeline(\n",
    "        impute.SimpleImputer(strategy=\"most_frequent\"),\n",
    "        preprocessing.OrdinalEncoder(\n",
    "            handle_unknown=\"use_encoded_value\", unknown_value=999\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "    transformers = [\n",
    "        (numeric_transformer, numerical_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "    ]\n",
    "\n",
    "    if additional_transformers is not None:\n",
    "        transformers.extend(additional_transformers)\n",
    "\n",
    "    preprocessor = compose.make_column_transformer(*transformers).set_output(\n",
    "        transform=\"pandas\"\n",
    "    )\n",
    "\n",
    "    model_pipeline = pipeline.make_pipeline(\n",
    "        preprocessor,\n",
    "        catboost.CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            eval_fraction=0.2,\n",
    "            early_stopping_rounds=20,\n",
    "            silent=True,\n",
    "            use_best_model=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "\n",
    "create_pipeline(NUMERICAL_FEATURES, CATEGORICAL_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b2b5b-4fa2-459a-9aee-a49543b161f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_validate(\n",
    "    estimator=create_pipeline(NUMERICAL_FEATURES, CATEGORICAL_FEATURES),\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "    cv=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc7df8-6b7d-47cd-b223-0f21f81d5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'OOF RMSE of basic pipeline : {np.mean(scores[\"test_neg_root_mean_squared_error\"])}'\n",
    ")\n",
    "print(f'OOF R2 of basic pipeline : {np.mean(scores[\"test_r2\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bd727-10ca-4812-982d-4d78c9b2429d",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "## Utilize categorical columns for grouping and transform each numerical variable based on the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b0bf9-f050-4939-af5c-e31249a43e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_feature, numerical_feature, transform_type):\n",
    "        self.categorical_feature = categorical_feature\n",
    "        self.numerical_feature = numerical_feature\n",
    "        self.transform_type = transform_type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate transformation of numerical_feature based on training data\n",
    "        self.transform_values_ = X.groupby(self.categorical_feature)[\n",
    "            self.numerical_feature\n",
    "        ].agg(self.transform_type)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Apply transformation to dataset\n",
    "        return X.assign(\n",
    "            CategoricalColumnTransformer=lambda df: df[self.categorical_feature].map(\n",
    "                self.transform_values_\n",
    "            )\n",
    "        )[[\"CategoricalColumnTransformer\"]]\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26107caa-ae40-45c5-a51a-b6317ba59bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "results = []\n",
    "for categorical_feature in tqdm(CATEGORICAL_FEATURES, desc=\"Progress\"):\n",
    "    for numerical_feature in NUMERICAL_FEATURES:\n",
    "        feature_adder = pipeline.make_pipeline(\n",
    "            impute.SimpleImputer(strategy=\"most_frequent\"),\n",
    "            CategoricalColumnTransformer(\n",
    "            categorical_feature=categorical_feature,\n",
    "            numerical_feature=numerical_feature,\n",
    "            transform_type=\"mean\",\n",
    "        )\n",
    "        )\n",
    "        additional_transformers = [\n",
    "            (feature_adder, [categorical_feature, numerical_feature])\n",
    "        ]\n",
    "        model_pipeline = create_pipeline(\n",
    "            numerical_features=NUMERICAL_FEATURES,\n",
    "            categorical_features=CATEGORICAL_FEATURES,\n",
    "            additional_transformers=additional_transformers,\n",
    "        )\n",
    "\n",
    "        scores = model_selection.cross_validate(\n",
    "            estimator=model_pipeline,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "            cv=10,\n",
    "        )\n",
    "\n",
    "        result = (\n",
    "            categorical_feature,\n",
    "            numerical_feature,\n",
    "            np.mean(scores[\"test_neg_root_mean_squared_error\"]),\n",
    "            np.mean(scores[\"test_r2\"]),\n",
    "        )\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe29589-3a2b-4790-9078-353e490a3d87",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| categorical_feature  | numerical_feature | mean_OOFs |\n",
    "| :---------------- | :------: | :----: |\n",
    "| building_condition | construction_year | 0.083537\t  |\n",
    "| province | bathrooms | 0.083706  | \n",
    "| heating_type | bedrooms | 0.083742  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31093121-8043-4683-9c5b-f6d2d0bb708b",
   "metadata": {},
   "source": [
    "## Generate bins from the continuous variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1cb57-11d1-4934-89df-4d0aa9160bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        continuous_feature_to_bin,\n",
    "        continuous_feature_to_transfer,\n",
    "        transform_type,\n",
    "        n_bins,\n",
    "    ):\n",
    "        self.continuous_feature_to_bin = continuous_feature_to_bin\n",
    "        self.continuous_feature_to_transfer = continuous_feature_to_transfer\n",
    "        self.transform_type = transform_type\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Determine bin edges based on training data\n",
    "        self.bin_edges_ = pd.qcut(\n",
    "            x=X[self.continuous_feature_to_bin],\n",
    "            q=self.n_bins,\n",
    "            retbins=True,\n",
    "            duplicates=\"drop\",\n",
    "        )[1]\n",
    "\n",
    "        # Calculate transformation of continuous_feature_to_transfer based on training data\n",
    "        self.transform_values_ = (\n",
    "            X.assign(\n",
    "                binned_continuous_feature=lambda df: pd.cut(\n",
    "                    df[self.continuous_feature_to_bin],\n",
    "                    bins=self.bin_edges_,\n",
    "                    labels=False,\n",
    "                )\n",
    "            )\n",
    "            .groupby(\"binned_continuous_feature\")[self.continuous_feature_to_transfer]\n",
    "            .agg(self.transform_type)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Apply binning and transformation to dataset\n",
    "        return X.assign(\n",
    "            binned_continuous_feature=lambda df: pd.cut(\n",
    "                df[self.continuous_feature_to_bin], bins=self.bin_edges_, labels=False\n",
    "            )\n",
    "        ).assign(\n",
    "            ContinuousColumnTransformer=lambda df: df[\"binned_continuous_feature\"].map(\n",
    "                self.transform_values_\n",
    "            )\n",
    "        )[\n",
    "            [\"ContinuousColumnTransformer\"]\n",
    "        ]\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d098e-3784-426d-ba85-43be5ee6bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "optimal_bins = int(np.floor(np.log2(X_train.shape[0])) + 1)\n",
    "results = []\n",
    "# Combine the loops to have a single progress bar\n",
    "for discretized_continuous in tqdm(NUMERICAL_FEATURES, desc=\"Progress\"):\n",
    "    for transformed_continuous in NUMERICAL_FEATURES:\n",
    "        if discretized_continuous != transformed_continuous:\n",
    "            \n",
    "            continuous_discretizer = pipeline.make_pipeline(\n",
    "                impute.SimpleImputer(strategy=\"median\"),\n",
    "                ContinuousColumnTransformer(\n",
    "                continuous_feature_to_bin=discretized_continuous,\n",
    "                continuous_feature_to_transfer=transformed_continuous,\n",
    "                transform_type=\"mean\",\n",
    "                n_bins=optimal_bins,\n",
    "            ))\n",
    "\n",
    "            additional_transformers = [\n",
    "                (\n",
    "                    continuous_discretizer,\n",
    "                    [discretized_continuous, transformed_continuous],\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            model_pipeline = create_pipeline(\n",
    "                numerical_features=NUMERICAL_FEATURES,\n",
    "                categorical_features=CATEGORICAL_FEATURES,\n",
    "                additional_transformers=additional_transformers,\n",
    "            )\n",
    "            scores = model_selection.cross_validate(\n",
    "                estimator=model_pipeline,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "                cv=10,\n",
    "            )\n",
    "            result = (\n",
    "                discretized_continuous,\n",
    "                transformed_continuous,\n",
    "                np.mean(scores[\"test_neg_root_mean_squared_error\"]),\n",
    "                np.mean(scores[\"test_r2\"]),\n",
    "            )\n",
    "            results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605337ef-7355-4b19-9114-7ee551c23be9",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| discretized_continuous  | transformed_continuous | mean_OOFs |\n",
    "| :---------------- | :------: | :----: |\n",
    "| bathrooms | number_of_frontages | 0.083759\t  |\n",
    "| bathrooms | zip_code | 0.083759  | \n",
    "| bathrooms | construction_year | 0.083759  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9954a88-a17e-437a-9e46-21c4ec1a6624",
   "metadata": {},
   "source": [
    "## Introduce polynomial features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16b391-afe7-4210-91db-196eb9146fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "results = []\n",
    "for numerical_feature in tqdm(NUMERICAL_FEATURES, desc=\"Progress\"):\n",
    "    \n",
    "    NEW_NUMERICAL_FEATURES = NUMERICAL_FEATURES.copy() \n",
    "    NEW_NUMERICAL_FEATURES.remove(numerical_feature)\n",
    "    \n",
    "    polyfeatures = pipeline.make_pipeline(\n",
    "                    impute.SimpleImputer(strategy=\"median\"),\n",
    "                    preprocessing.PolynomialFeatures(\n",
    "                        interaction_only=False, include_bias=False\n",
    "                    )\n",
    "                )\n",
    "    additional_transformers = [\n",
    "            (polyfeatures, [numerical_feature])\n",
    "        ]\n",
    "    \n",
    "    model_pipeline = create_pipeline(\n",
    "            numerical_features=NEW_NUMERICAL_FEATURES,\n",
    "            categorical_features=CATEGORICAL_FEATURES,\n",
    "            additional_transformers=additional_transformers,\n",
    "        )\n",
    "    \n",
    "    scores = model_selection.cross_validate(\n",
    "            estimator=model_pipeline,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "            cv=10,\n",
    "        )\n",
    "    \n",
    "    result = (\n",
    "            numerical_feature,\n",
    "            np.mean(scores[\"test_neg_root_mean_squared_error\"]),\n",
    "            np.mean(scores[\"test_r2\"]),\n",
    "        )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4cbd1a-04cb-4049-80e2-edf004dcf2e5",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| numerical_col  | mean_OOFs |\n",
    "| :---------------- | :------: |\n",
    "| bathrooms | 0.084046\t | \n",
    "| construction_year | 0.084068\t | \n",
    "| primary_energy_consumption | 0.084212| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fd630-d51d-4212-a78b-6f326e150696",
   "metadata": {},
   "source": [
    "## Implement other ideas derived from empirical observations or assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ee1f1-2b76-4b97-99e5-69c81367bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpiricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Apply transformation to dataset\n",
    "        return X.assign(\n",
    "            energy_efficiency=lambda df: df.primary_energy_consumption / df.living_area,\n",
    "            total_rooms=lambda df: df.bathrooms + df.bedrooms,\n",
    "            bedroom_to_bathroom=lambda df: df.bedrooms / df.bathrooms,\n",
    "            area_per_room=lambda df: df.living_area / df.bedrooms,\n",
    "            plot_to_livings_area=lambda df: df.surface_of_the_plot / df.living_area,\n",
    "        ).loc[:, \"energy_efficiency\":]\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacdd0d-3d18-4cbc-a0f1-a3b441a8ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "numeric_transformer = pipeline.make_pipeline(\n",
    "        impute.SimpleImputer(strategy=\"median\"), preprocessing.StandardScaler()\n",
    "    )\n",
    "\n",
    "categorical_transformer = pipeline.make_pipeline(\n",
    "    preprocessing.OrdinalEncoder(\n",
    "        handle_unknown=\"use_encoded_value\", unknown_value=999\n",
    "    ),\n",
    "    impute.SimpleImputer(strategy=\"median\"),\n",
    ")\n",
    "\n",
    "empirical_transformer = pipeline.make_pipeline(\n",
    "    EmpiricalTransformer(),\n",
    "    impute.SimpleImputer(strategy=\"median\"),\n",
    ")\n",
    "\n",
    "# Create a ColumnTransformer to handle both numerical and categorical features\n",
    "transformers = [\n",
    "    (numeric_transformer, NUMERICAL_FEATURES),\n",
    "    (categorical_transformer, CATEGORICAL_FEATURES),\n",
    "    (empirical_transformer, [\"primary_energy_consumption\", \n",
    "                             \"living_area\", \"bathrooms\", \n",
    "                             \"bedrooms\", \"surface_of_the_plot\",\n",
    "                            ])\n",
    "    \n",
    "]\n",
    "\n",
    "preprocessor = compose.make_column_transformer(*transformers).set_output(\n",
    "    transform=\"pandas\"\n",
    ")\n",
    "\n",
    "temp_dataframe = preprocessor.fit_transform(X_train)\n",
    "\n",
    "results = []\n",
    "for column in temp_dataframe.columns[-5:]:\n",
    "\n",
    "    temp_dataframe = preprocessor.fit_transform(X_train)\n",
    "    \n",
    "    added_features = temp_dataframe.columns[-5:].tolist()\n",
    "    features_to_remove = added_features.copy()  \n",
    "    features_to_remove.remove(column)\n",
    "\n",
    "    new_X_train = temp_dataframe.drop(columns = features_to_remove)\n",
    "\n",
    "    regressor = catboost.CatBoostRegressor(\n",
    "                iterations=100,\n",
    "                eval_fraction=0.2,\n",
    "                early_stopping_rounds=20,\n",
    "                silent=True,\n",
    "                use_best_model=True)\n",
    "    \n",
    "    scores = model_selection.cross_validate(\n",
    "            estimator=regressor,\n",
    "            X=new_X_train,\n",
    "            y=y_train,\n",
    "            scoring=(\"r2\", \"neg_root_mean_squared_error\"),\n",
    "            cv=10,\n",
    "        )\n",
    "    \n",
    "    result = (\n",
    "            column,\n",
    "            np.mean(scores[\"test_neg_root_mean_squared_error\"]),\n",
    "            np.mean(scores[\"test_r2\"]),\n",
    "        )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f893f0-9704-4d22-ad78-906a2343ac9e",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| feature  | mean_OOFs | \n",
    "| :---------------- | :------: | \n",
    "| pipeline-3__total_rooms\t | 0.084209\t | \n",
    "| pipeline-3__bedroom_to_bathroom\t | 0.084288\t | \n",
    "| pipeline-3__plot_to_livings_area\t | 0.084538|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcbc47-7df3-4f14-ad7c-753004e4c740",
   "metadata": {},
   "source": [
    "## Summarize the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecdb2b-4902-4a0e-8f70-963609fc1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data={\n",
    "        \"condition\": [\n",
    "            \"Utilize categorical columns for grouping\",\n",
    "            \"Generate bins from the continuous variables\",\n",
    "            \"Introduce polynomial features\",\n",
    "            \"Empirical observations\",\n",
    "            \"Original\",\n",
    "        ],\n",
    "        \"mean_OOFs\": [0.083537, 0.083759, 0.084046, 0.084209, 0.08467],\n",
    "    }\n",
    ").sort_values(by=\"mean_OOFs\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b18e4e-fb0f-48ac-9f61-67e54c4f6818",
   "metadata": {},
   "source": [
    "As you can see, with the exception of `Empirical observations`, the generated features scored better average validation RMSE values compared to the original setup, where no feature engineering applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57458324-a0e7-43a7-b3dd-c9cd7fbf0dbe",
   "metadata": {},
   "source": [
    "# Pipeline with feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244da8d-9df9-42e7-99bc-f7f803996611",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr, y_corr = preprocessing_transformers.remove_outliers(X_train, y_train)\n",
    "\n",
    "print(f\"Shape of X_train before correction: {X_train.shape}\")\n",
    "print(f\"Shape of X_train after correction: {X_corr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825f374-335f-4b2f-a185-b00a236bf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = model.create_tuned_pipeline(X_corr, y_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53124b-8474-49cc-a1e1-186397ae916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(\n",
    "    estimator=regressor,\n",
    "    X=X_corr,\n",
    "    y=y_corr,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=10,\n",
    ")\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540cc9e-555b-464c-920f-005370b583b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie_model = mapie.regression.MapieRegressor(regressor, method=\"base\", cv=5)\n",
    "# Fit the MAPIE model\n",
    "mapie_model.fit(X_corr, y_corr)\n",
    "joblib.dump(mapie_model, utils.Configuration.MODEL.joinpath(\"mapie_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a768be-653f-437e-8587-60b08d523b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie_model = joblib.load(utils.Configuration.MODEL.joinpath(\"mapie_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cdda6-4753-44df-866a-67d9b3588be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with prediction intervals on the transformed validation data\n",
    "y_pred, y_pis = mapie_model.predict(X_corr, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf789a0-29c6-44a3-8788-5559eb7d1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with y_valid and prediction intervals\n",
    "conformal_df = 10 ** pd.DataFrame(\n",
    "    {\n",
    "        \"y_test\": y_test,\n",
    "        \"lower\": y_pis[:, 0].flatten(),\n",
    "        \"upper\": y_pis[:, 1].flatten(),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by y_valid\n",
    "df_sorted = conformal_df.sort_values(by=\"y_test\")\n",
    "\n",
    "# Plot data\n",
    "\n",
    "plt.scatter(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"y_pred\"],\n",
    "    color=\"red\",\n",
    "    label=\"predicted\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.scatter(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"y_test\"],\n",
    "    color=\"green\",\n",
    "    label=\"ground truth\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.fill_between(\n",
    "    range(df_sorted.shape[0]),\n",
    "    df_sorted[\"lower\"],\n",
    "    df_sorted[\"upper\"],\n",
    "    alpha=0.2,\n",
    "    color=\"gray\",\n",
    "    label=\"Prediction Intervals\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded70432-2a8d-4e35-9fda-511df5bcdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    pd.DataFrame(\n",
    "        {\"2024-04-03\": 0.998, \"2024-04-04\": 0.878, \"2024-04-05\": 1.0},\n",
    "        index=np.arange(1),\n",
    "    )\n",
    "    .T.rename(columns={0: \"test\"})\n",
    "    .reset_index()\n",
    "    .pipe(lambda df: px.line(df, \"index\", \"test\", width=500))\n",
    ")\n",
    "\n",
    "fig.update_layout(yaxis_range=[0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
