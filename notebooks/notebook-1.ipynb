{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69412634-b259-4563-93c6-cf8064969b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import catboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import (compose, feature_selection, impute, metrics,\n",
    "                     model_selection, pipeline, preprocessing)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import creds\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c9950-3e8c-4b25-9774-558a068ab8e0",
   "metadata": {},
   "source": [
    "# Retrieve and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01bbdc-61df-4656-a86b-5aabfe45bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.retrieve_data_from_MongoDB({\"day_of_retrieval\": \"2024-02-03\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e1b2-3e78-4612-a3fe-a29a543093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.preprocess_and_split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d189685-e9ec-4de5-b55d-038cf64eb27c",
   "metadata": {},
   "source": [
    "# Assess preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385a8a3-af4e-4d2e-a8ea-5638b92c7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(zip_code=lambda df: pd.to_numeric(df.zip_code))\n",
    "\n",
    "# Extract numerical and categorical feature names\n",
    "NUMERICAL_FEATURES = X.select_dtypes(\"number\").columns.tolist()\n",
    "CATEGORICAL_FEATURES = X.select_dtypes(\"object\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68690570-37f7-432c-a637-b583ff08b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define transformers for preprocessing\n",
    "# numeric_transformer = pipeline.make_pipeline(impute.SimpleImputer(strategy=\"median\"))\n",
    "\n",
    "# categorical_transformer = pipeline.make_pipeline(\n",
    "#     preprocessing.OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "#     impute.SimpleImputer(strategy=\"median\"),\n",
    "# )\n",
    "\n",
    "# # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "# preprocessor = compose.make_column_transformer(\n",
    "#     (numeric_transformer, NUMERICAL_FEATURES),\n",
    "#     (categorical_transformer, CATEGORICAL_FEATURES),\n",
    "# )\n",
    "\n",
    "# # Create a pipeline that includes the preprocessor and the model\n",
    "# model_pipeline = pipeline.make_pipeline(\n",
    "#     preprocessor, catboost.CatBoostRegressor(iterations=10, silent=True)\n",
    "# )\n",
    "\n",
    "# scores = model_selection.cross_val_score(\n",
    "#     estimator=model_pipeline, X=X, y=y, scoring=\"neg_root_mean_squared_error\", cv=10\n",
    "# )\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f4c4f-90bc-4b76-9a60-f0132c41483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to enable target encoding within a pipeline\n",
    "\n",
    "\n",
    "class TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **te_args):\n",
    "        self.te_args = te_args\n",
    "        self.encoder = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.encoder = preprocessing.TargetEncoder(**self.te_args)\n",
    "        self.encoder.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X)\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    preprocessing.OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=999),\n",
    "    TargetEncoderWrapper(target_type=\"continuous\"),\n",
    "]\n",
    "imputers = [impute.SimpleImputer(strategy=\"median\"), impute.KNNImputer()]\n",
    "scalers = [\n",
    "    preprocessing.StandardScaler(),\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    preprocessing.RobustScaler(),\n",
    "]\n",
    "\n",
    "# Extract numerical and categorical feature names\n",
    "NUMERICAL_FEATURES = X.select_dtypes(\"number\").columns.tolist()\n",
    "CATEGORICAL_FEATURES = X.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over the imputers, encoders and scalers\n",
    "for imputer in imputers:\n",
    "    for encoder in encoders:\n",
    "        for scaler in scalers:\n",
    "            # Define transformers for preprocessing\n",
    "            numeric_transformer = pipeline.make_pipeline(imputer, scaler)\n",
    "            categorical_transformer = pipeline.make_pipeline(encoder, imputer)\n",
    "\n",
    "            # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "            preprocessor = compose.make_column_transformer(\n",
    "                (numeric_transformer, NUMERICAL_FEATURES),\n",
    "                (categorical_transformer, CATEGORICAL_FEATURES),\n",
    "            )\n",
    "            model_pipeline = pipeline.make_pipeline(\n",
    "                preprocessor, catboost.CatBoostRegressor(iterations=100, silent=True)\n",
    "            )\n",
    "            scores = model_selection.cross_val_score(\n",
    "                estimator=model_pipeline,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                scoring=\"neg_root_mean_squared_error\",\n",
    "                cv=10,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                \"imputer\": str(imputer),\n",
    "                \"encoder\": str(encoder),\n",
    "                \"scaler\": str(scaler),\n",
    "                \"scores\": scores.tolist(),\n",
    "                \"mean_score\": np.mean(scores),\n",
    "                \"std_score\": np.std(scores),\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "pd.DataFrame(results).sort_values(by=\"mean_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c2556-0b08-4359-97dc-2a8175d2ac0c",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| List of preprocessors  | AVG RMSE on Validation set | STD of RMSEs on Validation set |\n",
    "| :---------------- | :------: | :----: |\n",
    "| SimpleImputer, OneHotEncoder, MinMaxScaler | 0.099271 | 0.004467  |\n",
    "| SimpleImputer, OneHotEncoder, StandardScaler |  0.099285\t | 0.004499  |\n",
    "| SimpleImputer, TargetEncoderWrapper, MinMaxScaler | 0.099382 |  0.003905 |\n",
    "\n",
    "\n",
    "Based on this experiment we can use the `SimpleImputer` with median strategy and `MinMaxScaler` for *numerical features* and `OneHotEncoder` (handle_unknown set to \"ignore\" and sparse_output to False) and `SimpleImputer` with median strategy for *non-numerical columns*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a356e4-deeb-4f4a-9b48-b35bc1274451",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "## Utilize categorical columns for grouping and transform each numerical variable based on the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f26a3-b107-47cf-91c1-1e8318a2163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_categorical_transform(\n",
    "    X: pd.DataFrame, y: pd.Series, transform_type: str = \"mean\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function performs feature engineering on a given DataFrame by transforming categorical features\n",
    "    based on a specified transformation type (mean, sum, min, max) applied to numerical features.\n",
    "    The transformed features are then used to train a CatBoostRegressor model and the performance is evaluated\n",
    "    using cross-validation. The results are returned as a DataFrame sorted by mean out-of-fold scores.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input DataFrame containing the features.\n",
    "    y (pd.Series): The target variable series.\n",
    "    transform_type (str, optional): The type of transformation to apply to each group within the categorical column.\n",
    "                                    It can be one of the following: 'mean', 'sum', 'min', 'max'. Default is 'mean'.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the transform_type is not one of the following: 'mean', 'sum', 'min', 'max'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the categorical feature, numerical feature, mean out-of-fold scores,\n",
    "                  and standard deviation of out-of-fold scores, sorted by mean out-of-fold scores in descending order.\n",
    "    \"\"\"\n",
    "    if transform_type not in [\"mean\", \"sum\", \"min\", \"max\"]:\n",
    "        raise ValueError(f\"Invalid transform_type: {transform_type}\")\n",
    "\n",
    "    results = []\n",
    "    for categorical in tqdm(CATEGORICAL_FEATURES, desc=\"Progress\"):\n",
    "        for numerical in NUMERICAL_FEATURES:\n",
    "            # Create a deep copy of the input data\n",
    "            temp = X.copy(deep=True)\n",
    "\n",
    "            # Calculate the transformation for each group within the categorical column\n",
    "            temp[\"new_column\"] = temp.groupby(categorical)[numerical].transform(\n",
    "                transform_type\n",
    "            )\n",
    "\n",
    "            new_numerical_features = temp.select_dtypes(\"number\").columns.tolist()\n",
    "            new_categorical_features = temp.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "            # Define transformers for preprocessing\n",
    "            numeric_transformer = pipeline.make_pipeline(\n",
    "                impute.SimpleImputer(strategy=\"median\"), preprocessing.MinMaxScaler()\n",
    "            )\n",
    "\n",
    "            categorical_transformer = pipeline.make_pipeline(\n",
    "                preprocessing.OneHotEncoder(\n",
    "                    handle_unknown=\"ignore\", sparse_output=False\n",
    "                ),\n",
    "                impute.SimpleImputer(strategy=\"median\"),\n",
    "            )\n",
    "\n",
    "            # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "            preprocessor = compose.make_column_transformer(\n",
    "                (numeric_transformer, new_numerical_features),\n",
    "                (categorical_transformer, new_categorical_features),\n",
    "            ).set_output(transform=\"pandas\")\n",
    "\n",
    "            model_pipeline = pipeline.make_pipeline(\n",
    "                preprocessor, catboost.CatBoostRegressor(iterations=100, silent=True)\n",
    "            )\n",
    "\n",
    "            scores = model_selection.cross_val_score(\n",
    "                estimator=model_pipeline,\n",
    "                X=temp,\n",
    "                y=y,\n",
    "                scoring=\"neg_root_mean_squared_error\",\n",
    "                cv=10,\n",
    "            )\n",
    "\n",
    "            # Store the results as a tuple\n",
    "            result = (categorical, numerical, np.mean(scores), np.std(scores))\n",
    "            results.append(result)\n",
    "\n",
    "            del temp, new_numerical_features, new_categorical_features, scores, result\n",
    "\n",
    "    # Create a DataFrame from the results and sort it by mean OOF scores\n",
    "    result_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\"categorical_feature\", \"numerical_feature\", \"mean_OOFs\", \"std_OOFs\"],\n",
    "    )\n",
    "    result_df = result_df.sort_values(by=\"mean_OOFs\", ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d5e2b-0524-4d4a-9129-79c6b65d9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_FE_categorical_transform = FE_categorical_transform(X, y)\n",
    "df_FE_categorical_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96713709-9423-4ad2-bfc4-29ccbfb2095c",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| categorical_feature  | numerical_feature | mean_OOFs | std_OOFs|\n",
    "| :---------------- | :------: | :----: | :----: |\n",
    "| province | living_area | -0.099005  | 0.004469 |\n",
    "| heating_type | living_area | -0.099054  | 0.003247 |\n",
    "| building_condition | primary_energy_consumption | -0.099086  |0.003556 |\n",
    "\n",
    "\n",
    "The best result was obtained by taking the `province` feature as categorical variable and calculating the mean of `living_area`. The resulting OOF RMSE is *0.099005* which is slightly better than our base model (*0.099271*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31093121-8043-4683-9c5b-f6d2d0bb708b",
   "metadata": {},
   "source": [
    "## Generate bins from the continuous variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee8786-d5f6-4266-bb47-9155612f212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_continuous_transform(\n",
    "    X: pd.DataFrame, y: pd.Series, transform_type: str = \"mean\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function performs feature engineering on a given DataFrame by transforming continuous features\n",
    "    based on a specified transformation type (mean, sum, min, max) applied to other continuous features.\n",
    "    The transformed features are then used to train a CatBoostRegressor model and the performance is evaluated\n",
    "    using cross-validation. The results are returned as a DataFrame sorted by mean out-of-fold scores.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The input DataFrame containing the features.\n",
    "    y (pd.Series): The target variable series.\n",
    "    transform_type (str, optional): The type of transformation to apply to each group within the discretized continuous column.\n",
    "                                    It can be one of the following: 'mean', 'sum', 'min', 'max'. Default is 'mean'.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the transform_type is not one of the following: 'mean', 'sum', 'min', 'max'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the discretized continuous feature, transformed continuous feature, mean out-of-fold scores,\n",
    "                  and standard deviation of out-of-fold scores, sorted by mean out-of-fold scores in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    if transform_type not in [\"mean\", \"sum\", \"min\", \"max\"]:\n",
    "        raise ValueError(f\"Invalid transform_type: {transform_type}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Get a list of continuous and numerical columns\n",
    "    continuous_columns = [\n",
    "        \"zip_code\",\n",
    "        \"primary_energy_consumption\",\n",
    "        \"living_area\",\n",
    "        \"surface_of_the_plot\",\n",
    "        \"construction_year\",\n",
    "    ]\n",
    "    optimal_bins = int(np.floor(np.log2(X.shape[0])) + 1)\n",
    "\n",
    "    # Combine the loops to have a single progress bar\n",
    "    for discretized_continuous in tqdm(continuous_columns, desc=\"Progress:\"):\n",
    "        for transformed_continuous in continuous_columns:\n",
    "            if discretized_continuous != transformed_continuous:\n",
    "                # Create a deep copy of the input data\n",
    "                temp = X.copy(deep=True)\n",
    "\n",
    "                discretizer = pipeline.make_pipeline(\n",
    "                    impute.SimpleImputer(strategy=\"median\"),\n",
    "                    preprocessing.KBinsDiscretizer(\n",
    "                        encode=\"ordinal\", n_bins=optimal_bins\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                temp[discretized_continuous] = discretizer.fit_transform(\n",
    "                    X[[discretized_continuous]]\n",
    "                )\n",
    "\n",
    "                # Calculate the transformation for each group within the categorical column\n",
    "                temp[\"new_column\"] = temp.groupby(discretized_continuous)[\n",
    "                    transformed_continuous\n",
    "                ].transform(transform_type)\n",
    "\n",
    "                new_numerical_features = temp.select_dtypes(\"number\").columns.tolist()\n",
    "                new_categorical_features = temp.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "                # Define transformers for preprocessing\n",
    "                numeric_transformer = pipeline.make_pipeline(\n",
    "                    impute.SimpleImputer(strategy=\"median\"),\n",
    "                    preprocessing.MinMaxScaler(),\n",
    "                )\n",
    "\n",
    "                categorical_transformer = pipeline.make_pipeline(\n",
    "                    preprocessing.OneHotEncoder(\n",
    "                        handle_unknown=\"ignore\", sparse_output=False\n",
    "                    ),\n",
    "                    impute.SimpleImputer(strategy=\"median\"),\n",
    "                )\n",
    "\n",
    "                # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "                preprocessor = compose.make_column_transformer(\n",
    "                    (numeric_transformer, new_numerical_features),\n",
    "                    (categorical_transformer, new_categorical_features),\n",
    "                ).set_output(transform=\"pandas\")\n",
    "\n",
    "                model_pipeline = pipeline.make_pipeline(\n",
    "                    preprocessor,\n",
    "                    catboost.CatBoostRegressor(iterations=100, silent=True),\n",
    "                )\n",
    "\n",
    "                scores = model_selection.cross_val_score(\n",
    "                    estimator=model_pipeline,\n",
    "                    X=temp,\n",
    "                    y=y,\n",
    "                    scoring=\"neg_root_mean_squared_error\",\n",
    "                    cv=10,\n",
    "                )\n",
    "\n",
    "                # Store the results as a tuple\n",
    "                result = (\n",
    "                    discretized_continuous,\n",
    "                    transformed_continuous,\n",
    "                    np.mean(scores),\n",
    "                    np.std(scores),\n",
    "                )\n",
    "                results.append(result)\n",
    "\n",
    "                del temp, scores, result\n",
    "\n",
    "    # Create a DataFrame from the results and sort it by mean OOF scores\n",
    "    result_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"discretized_continuous\",\n",
    "            \"transformed_continuous\",\n",
    "            \"mean_OOFs\",\n",
    "            \"std_OOFs\",\n",
    "        ],\n",
    "    )\n",
    "    result_df = result_df.sort_values(by=\"mean_OOFs\", ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be390fe3-9c36-4d33-9fc1-c8e5544106f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_FE_continuous_transform = FE_continuous_transform(X, y)\n",
    "df_FE_continuous_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605337ef-7355-4b19-9114-7ee551c23be9",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| discretized_continuous  | transformed_continuous | mean_OOFs | std_OOFs|\n",
    "| :---------------- | :------: | :----: | :----: |\n",
    "| primary_energy_consumption | zip_code | -0.099286\t  |0.0034879 |\n",
    "| primary_energy_consumption | living_area | -0.099351  | 0.003779 |\n",
    "| primary_energy_consumption | surface_of_the_plot | -0.099562  |0.003809 |\n",
    "\n",
    "\n",
    "The best result was obtained by taking the `0.003809` feature as discretized continuous variable and calculating the mean of `zip_code`. The resulting OOF RMSE is *0.099286* which is slightly worse than our base model (*0.099271*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9954a88-a17e-437a-9e46-21c4ec1a6624",
   "metadata": {},
   "source": [
    "## Introduce polynomial features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bf4c6-a49f-49d7-a53b-af680af4e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_polynomial_features(\n",
    "    X: pd.DataFrame, y: pd.Series, combinations: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    # Get a list of continuous and numerical columns\n",
    "    numerical_columns = X.select_dtypes(\"number\").columns\n",
    "\n",
    "    # Combine the loops to have a single progress bar\n",
    "    for numerical_col in tqdm(\n",
    "        list(itertools.combinations(numerical_columns, r=combinations))\n",
    "    ):\n",
    "        polyfeatures = compose.make_column_transformer(\n",
    "            (\n",
    "                pipeline.make_pipeline(\n",
    "                    impute.SimpleImputer(strategy=\"median\"),\n",
    "                    preprocessing.PolynomialFeatures(\n",
    "                        interaction_only=False, include_bias=False\n",
    "                    ),\n",
    "                ),\n",
    "                list(numerical_col),\n",
    "            ),\n",
    "            remainder=\"passthrough\",\n",
    "        ).set_output(transform=\"pandas\")\n",
    "\n",
    "        temp = polyfeatures.fit_transform(X)\n",
    "\n",
    "        new_numerical_features = temp.select_dtypes(\"number\").columns.tolist()\n",
    "        new_categorical_features = temp.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "        # Define transformers for preprocessing\n",
    "        numeric_transformer = pipeline.make_pipeline(\n",
    "            impute.SimpleImputer(strategy=\"median\"), preprocessing.MinMaxScaler()\n",
    "        )\n",
    "\n",
    "        categorical_transformer = pipeline.make_pipeline(\n",
    "            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            impute.SimpleImputer(strategy=\"median\"),\n",
    "        )\n",
    "\n",
    "        # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "        preprocessor = compose.make_column_transformer(\n",
    "            (numeric_transformer, new_numerical_features),\n",
    "            (categorical_transformer, new_categorical_features),\n",
    "        ).set_output(transform=\"pandas\")\n",
    "\n",
    "        model_pipeline = pipeline.make_pipeline(\n",
    "            preprocessor, catboost.CatBoostRegressor(iterations=100, silent=True)\n",
    "        )\n",
    "\n",
    "        scores = model_selection.cross_val_score(\n",
    "            estimator=model_pipeline,\n",
    "            X=temp,\n",
    "            y=y,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=10,\n",
    "        )\n",
    "\n",
    "        # Store the results as a tuple\n",
    "        result = (numerical_col, np.mean(scores), np.std(scores))\n",
    "        results.append(result)\n",
    "\n",
    "        del temp, scores, result\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        results, columns=[\"numerical_col\", \"mean_OOFs\", \"std_OOFs\"]\n",
    "    )\n",
    "    result_df = result_df.sort_values(by=\"mean_OOFs\", ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4a47f-a7f9-49a7-88f3-231a650fcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "FE_polynomial_features(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f967b-3046-4d00-8fef-08b3701b3fd2",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| numerical_col  | mean_OOFs | std_OOFs |\n",
    "| :---------------- | :------: | :----: |\n",
    "| zip_code | -0.098392\t | 0.004372\t  |\n",
    "| bedrooms | -0.098816\t | 0.003782 |\n",
    "| primary_energy_consumption | -0.098900| 0.004057 |\n",
    "\n",
    "\n",
    "The best result was obtained by taking the polynomial feature of the `zip_code`. The resulting OOF RMSE is *0.098392* which is slightly better than our base model (*0.099271*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fd630-d51d-4212-a78b-6f326e150696",
   "metadata": {},
   "source": [
    "## Implement other ideas derived from empirical observations or assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa00e1a-ffa6-45f4-a682-b1f85ebb4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_ideas(X):\n",
    "    \"\"\"Performs additional feature engineering on the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The input DataFrame containing the original features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with additional engineered features.\n",
    "\n",
    "    Example:\n",
    "        >>> engineered_data = FE_ideas(original_data)\n",
    "    \"\"\"\n",
    "    temp = X.assign(\n",
    "        energy_efficiency=lambda df: df.primary_energy_consumption / df.living_area,\n",
    "        total_rooms=lambda df: df.bathrooms + df.bedrooms,\n",
    "        bedroom_to_bathroom=lambda df: df.bedrooms / df.bathrooms,\n",
    "        area_per_room=lambda df: df.living_area / df.bedrooms,\n",
    "        plot_to_livings_area=lambda df: df.surface_of_the_plot / df.living_area,\n",
    "    )\n",
    "    return temp.loc[:, \"energy_efficiency\":]\n",
    "\n",
    "\n",
    "FE_ideas(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0380ea-9d02-4ae5-9092-9a9afc1bb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FE_try_ideas(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Performs feature engineering experiments by adding new features and evaluating their impact on model performance.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The input feature matrix.\n",
    "        y (pd.Series): The target variable.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the results of feature engineering experiments.\n",
    "\n",
    "    Example:\n",
    "        >>> results_df = FE_try_ideas(X, y)\n",
    "    \"\"\"\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # Apply additional feature engineering ideas\n",
    "    feature_df = FE_ideas(X)\n",
    "\n",
    "    for feature in tqdm(feature_df.columns):\n",
    "        # Concatenate the original features with the newly engineered feature\n",
    "        temp = pd.concat([X, feature_df[feature]], axis=\"columns\")\n",
    "\n",
    "        new_numerical_features = temp.select_dtypes(\"number\").columns.tolist()\n",
    "        new_categorical_features = temp.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "        # Define transformers for preprocessing\n",
    "        numeric_transformer = pipeline.make_pipeline(\n",
    "            impute.SimpleImputer(strategy=\"median\"), preprocessing.MinMaxScaler()\n",
    "        )\n",
    "\n",
    "        categorical_transformer = pipeline.make_pipeline(\n",
    "            preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            impute.SimpleImputer(strategy=\"median\"),\n",
    "        )\n",
    "\n",
    "        # Create a ColumnTransformer to handle both numerical and categorical features\n",
    "        preprocessor = compose.make_column_transformer(\n",
    "            (numeric_transformer, new_numerical_features),\n",
    "            (categorical_transformer, new_categorical_features),\n",
    "        ).set_output(transform=\"pandas\")\n",
    "\n",
    "        model_pipeline = pipeline.make_pipeline(\n",
    "            preprocessor, catboost.CatBoostRegressor(iterations=100, silent=True)\n",
    "        )\n",
    "\n",
    "        scores = model_selection.cross_val_score(\n",
    "            estimator=model_pipeline,\n",
    "            X=temp,\n",
    "            y=y,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=10,\n",
    "        )\n",
    "\n",
    "        # Store the results as a tuple\n",
    "        result = (feature, np.mean(scores), np.std(scores))\n",
    "        results.append(result)\n",
    "\n",
    "        del temp, scores, result\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=[\"feature\", \"mean_OOFs\", \"std_OOFs\"])\n",
    "    result_df = result_df.sort_values(by=\"mean_OOFs\", ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9659a50-c1db-419f-a336-dd41667065d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "df_FE_try_ideas = FE_try_ideas(X,y)\n",
    "df_FE_try_ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a150b4-7c89-4da6-af5d-ce254f9a8605",
   "metadata": {},
   "source": [
    "**Best performances :**\n",
    "\n",
    "| feature  | mean_OOFs | std_OOFs |\n",
    "| :---------------- | :------: | :----: |\n",
    "| energy_efficiency | -0.099122\t | 0.004271|\n",
    "| area_per_room | -0.099712\t | 0.003463 |\n",
    "| plot_to_livings_area | -0.099972| 0.004570 |\n",
    "\n",
    "\n",
    "The best result was obtained was the incorporation of the `energy_efficiency` feature. The resulting OOF RMSE is *0.099122* which is slightly better than our base model (*0.099271*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcbc47-7df3-4f14-ad7c-753004e4c740",
   "metadata": {},
   "source": [
    "## Summarize the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecdb2b-4902-4a0e-8f70-963609fc1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data={\n",
    "        \"condition\": [\n",
    "            \"Utilize categorical columns for grouping\",\n",
    "            \"Generate bins from the continuous variables\",\n",
    "            \"Introduce polynomial features\",\n",
    "            \"Empirical observations\",\n",
    "            \"Original\",\n",
    "        ],\n",
    "        \"mean_OOFs\": [0.099005, 0.099286, 0.098392, 0.099122, 0.099271],\n",
    "    }\n",
    ").sort_values(by=\"mean_OOFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b18e4e-fb0f-48ac-9f61-67e54c4f6818",
   "metadata": {},
   "source": [
    "As you can see, with the exception of `Generate bins from the continuous variables`, the generated features scored better average validation RMSE values compared to the original setup, where no feature engineering applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57458324-a0e7-43a7-b3dd-c9cd7fbf0dbe",
   "metadata": {},
   "source": [
    "## Final feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38f507-213a-4ada-8aa2-10f74bf1f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.assign(\n",
    "            energy_efficiency=lambda df: X.primary_energy_consumption / X.living_area,\n",
    "            tr_prov_liv=X.groupby(\"province\")[\"living_area\"].transform(\"mean\"),\n",
    "            tr_heat_liv=X.groupby(\"heating_type\")[\"living_area\"].transform(\"mean\"),\n",
    "            tr_buil_energy=X.groupby(\"building_condition\")[\n",
    "                \"primary_energy_consumption\"\n",
    "            ].transform(\"mean\"),\n",
    "        )[[\"energy_efficiency\", \"tr_prov_liv\", \"tr_heat_liv\", \"tr_buil_energy\"]]\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce03a10-fed1-4b8c-9096-298c94a1f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_numerical_features = [\n",
    "    item\n",
    "    for item in X.select_dtypes(\"number\").columns.tolist()\n",
    "    if item not in [\"zip_code\", \"bedrooms\", \"primary_energy_consumption\"]\n",
    "]\n",
    "new_categorical_features = X.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "polyfeatures = pipeline.make_pipeline(\n",
    "    impute.SimpleImputer(strategy=\"median\"),\n",
    "    preprocessing.PolynomialFeatures(interaction_only=False, include_bias=False),\n",
    "    preprocessing.MinMaxScaler(),\n",
    ")\n",
    "\n",
    "feature_adder = pipeline.make_pipeline(\n",
    "    FeatureAdder(),\n",
    "    impute.SimpleImputer(strategy=\"median\"),\n",
    "    preprocessing.MinMaxScaler(),\n",
    ")\n",
    "\n",
    "# Define transformers for preprocessing\n",
    "numeric_transformer = pipeline.make_pipeline(\n",
    "    impute.SimpleImputer(strategy=\"median\"), preprocessing.MinMaxScaler()\n",
    ")\n",
    "\n",
    "categorical_transformer = pipeline.make_pipeline(\n",
    "    preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    impute.SimpleImputer(strategy=\"median\"),\n",
    ")\n",
    "\n",
    "# Create a ColumnTransformer to handle both numerical and categorical features\n",
    "preprocessor = compose.make_column_transformer(\n",
    "    (polyfeatures, [\"zip_code\", \"bedrooms\", \"primary_energy_consumption\"]),\n",
    "    (\n",
    "        feature_adder,\n",
    "        [\n",
    "            \"primary_energy_consumption\",\n",
    "            \"living_area\",\n",
    "            \"province\",\n",
    "            \"heating_type\",\n",
    "            \"building_condition\",\n",
    "        ],\n",
    "    ),\n",
    "    (numeric_transformer, new_numerical_features),\n",
    "    (categorical_transformer, new_categorical_features),\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# model_pipeline = pipeline.make_pipeline(\n",
    "#     preprocessor, catboost.CatBoostRegressor(iterations=100, silent=True)\n",
    "# )\n",
    "\n",
    "# scores = model_selection.cross_val_score(\n",
    "#     estimator=model_pipeline,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     scoring=\"neg_root_mean_squared_error\",\n",
    "#     cv=10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dde883-cd3a-44aa-ba1a-a778ea054721",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05e033-a9dc-4d8f-a2db-89e9f3ad8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for n_features in tqdm(range(2, 55)):\n",
    "    model_pipeline = pipeline.make_pipeline(\n",
    "        preprocessor,\n",
    "        feature_selection.SelectFromModel(\n",
    "            catboost.CatBoostRegressor(iterations=100, silent=True),\n",
    "            max_features=n_features,\n",
    "        ),\n",
    "        catboost.CatBoostRegressor(iterations=100, silent=True),\n",
    "    )\n",
    "\n",
    "    # Get the names of the features selected\n",
    "    model_pipeline.fit(X, y)\n",
    "    selected_names = model_pipeline.named_steps[\n",
    "        \"selectfrommodel\"\n",
    "    ].get_feature_names_out()\n",
    "\n",
    "    scores = model_selection.cross_val_score(\n",
    "        estimator=model_pipeline,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=10,\n",
    "    )\n",
    "    result = (n_features, selected_names, np.mean(scores), np.std(scores))\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db40b70-570b-4c52-8672-b655978112c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=[\"n_features\", \"selected_names\", \"mean_OOF\", \"STD_OOF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f915357-147e-475e-ad15-c1b9eb5dbaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6521fd-aadc-4a5e-b5e4-53821434dbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c6d42-2069-4e8a-8ca5-1d216a9163db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b25c65-b69d-4500-bb10-2d611b8694df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
